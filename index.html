<!DOCTYPE html>
<html lang="en">
  <head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-23967655-67"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-23967655-67');
</script>
    <meta charset="utf-8">
    <title>Foundations of Machine Learning</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
    <link rel="stylesheet" href="styles/style.css">
    <link rel="stylesheet" media="only screen and (max-width: 770px)" href="styles/tablet-and-phone.css">
    <link rel="stylesheet" media="only screen and (max-width: 420px)" href="styles/phone.css">
    <link rel="icon" href="favicon.ico" type="image/vnd.microsoft.icon">
    <link rel="canonical" href="https://bloomberg.github.io/foml/">
  </head>
  <body>
    <nav>
        <a href="#home">Home</a>
        <a href="#about">About</a>
        <a href="#lectures">Lectures</a>
        <a href="#assignments">Assignments</a>
        <a href="#resources">Resources</a>
        <a href="#people">People</a>
    </nav>

    <section id="home">
        <a href="https://www.techatbloomberg.com/post-topic/data-science/"><img src="images/mlbanner.jpg" alt="Bloomberg ML EDU presents:"></a>
        <h1>Foundations of Machine Learning</h1>
        <table id="course-info">
            <tr>
                <th>Instructor</th>
                <td><a href="#people">David S. Rosenberg</a>, Office of the CTO at Bloomberg</td>
            </tr>
        </table>

        <p id="course-pitch"><strong>Understand the Concepts, Techniques and Mathematical Frameworks Used by Experts in Machine Learning</strong></p>
    </section>
    <section id="about">
        <h1>About This Course</h1>

        <div class="module">
            <p>Bloomberg presents "Foundations of Machine Learning," a training course that was initially delivered internally to the company's software engineers as part of its "Machine Learning EDU" initiative. This course covers a wide variety of topics in machine learning and statistical modeling. The primary goal of the class is to help participants gain a deep understanding of the concepts, techniques and mathematical frameworks used by experts in machine learning. It is designed to make valuable machine learning skills more accessible to individuals with a strong math background, including software developers, experimental scientists, engineers and financial professionals.</p>

            <p>The 30 lectures in the course are embedded below, but may also be viewed in this <a href="https://www.youtube.com/playlist?list=PLnZuxOufsXnvftwTB1HL6mel1V32w0ThI">YouTube playlist</a>. The course includes a complete set of homework assignments, each containing a theoretical element and implementation challenge with support code in Python, which is rapidly becoming the prevailing programming language for data science and machine learning in both academia and industry. This course also serves as a foundation on which more specialized courses and further independent study can build.</p>

            <p>Please fill out <a href="https://docs.google.com/forms/d/e/1FAIpQLSeyq3l0U3SOX5km78Bg_JcRZWg5XtWpy3n5dEw3kbt3YudIZw/viewform?usp=sf_link">this short online form</a> to register for access to our course's <a href="https://piazza.com/">Piazza</a> discussion board. Applications are processed manually, so please be patient.  You should receive an email directly from Piazza when you are registered. Common questions from this and previous editions of the course are posted in our <a href="https://github.com/davidrosenberg/mlcourse/blob/gh-pages/course-faq.md">FAQ</a>.</p>

            <!-- Without registering, you can also view an <a href="https://piazza.com/class/i2jg9qgaxwr5fq?cid=14">anonymized version of our Piazza board</a>.</p> -->

            <p>The first lecture, <a href="#lecture-black-box-machine-learning">Black Box Machine Learning</a>, gives a quick start introduction to practical machine learning and  only requires familiarity with basic programming concepts.</p>

<!--  
            <section>
                <h1>Highlights and Distinctive Features of the Course Lectures, Notes, and Assignments</h1>

                <ul>
                    <li>Geometric explanation for what happens with ridge, lasso, and elastic net regression in the case of correlated random variables.</li>
                    <li>Investigation of when the penalty (Tikhonov) and constraint (Ivanov) forms of regularization are equivalent.</li>
                    <li>Concise summary of what we really learn about SVMs from Lagrangian duality.</li>
                    <li>Proof of representer theorem with simple linear algebra, emphasizing it as a way to reparametrize certain objective functions.</li>
                    <li>Guided derivation of the math behind the classic diamond/circle/ellipsoids picture that "explains" why L1 regularization gives sparsity (Homework 2, Problem 5)</li>
                    <li>From scrach (in numpy) implementation of almost all major ML algorithms we discuss: ridge regression with SGD and GD (Homework 1, Problems 2.5, 2.6 page 4), lasso regression with the shooting algorithm (Homework 2, Problem 3, page 4), kernel ridge regression (Homework 4, Problem 3, page 2), kernelized SVM with Kernelized Pegasos (Homework 4, 6.4, page 9), L2-regularized logistic regression (Homework 5, Problem 3.3, page 4),Bayesian Linear Regession (Homework 5, problem 5, page 6), multiclass SVM (Homework 6, Problem 4.2, p. 3), classification and regression trees (without pruning)  (Homework 6, Problem 6), gradient boosting with trees for classification and regression (Homework 6, Problem 8), multilayer perceptron for regression (Homework 7, Problem 4, page 3)</li>
                    <li>Repeated use of a simple 1-dimensional regression dataset, so it's easy to visualize the effect of various hypothesis spaces and regularizations that we investigate throughout the course.</li>
                    <li>Investigation of how to derive a conditional probability estimate from a predicted score for various loss functions, and why it's not so straightforward for the hinge loss (i.e. the SVM) (Homework 5, Problem 2, page 1)</li>
                    <li>Discussion of numerical overflow issues and the log-sum-exp trick (Homework 5, Problem 3.2)</li>
                    <li>Self-contained introduction to the expectation maximization (EM) algorithm for latent variable models.</li>
                    <li>Develop a general computation graph framework from scratch, using numpy, and implement your neural networks in it.</li>
                </ul>
            </section>
-->
            <section>
                <h1>Prerequisites</h1>

                <p>The quickest way to see if the mathematics level of the course is for you is to take a look at this <a href="https://davidrosenberg.github.io/mlcourse/Notes/prereq-questions/math-questions.pdf">mathematics assessment</a>, which is a preview of some of the math concepts that show up in the first part of the course.</p>

                <ul>
                    <li><strong>Solid mathematical background</strong>, equivalent to a 1-semester undergraduate course in each of the following: linear algebra, multivariate differential calculus, probability theory, and statistics. The content of NYU's <a href="http://www.cims.nyu.edu/~cfgranda/pages/DSGA1002_fall15/index.html"><strong>DS-GA-1002: Statistical and Mathematical Methods</strong></a> would be more than sufficient, for example.</li>
                    <li><strong>Python programming required</strong> for most homework assignments.</li>
                    <li><em>Recommended:</em> At least one advanced, proof-based mathematics course</li>
                    <li><em>Recommended:</em> Computer science background up to a "data structures and algorithms" course</li>
                </ul>
            </section>
        </div>
    </section>

    <section id="lectures">
        <h1>Lectures</h1>

        <ul class="abbreviations">
            <li>(HTF) refers to Hastie, Tibshirani, and Friedman's book <a href="https://web.stanford.edu/~hastie/ElemStatLearn/"><cite>The Elements of Statistical Learning</cite></a></li>
            <li>(SSBD) refers to Shalev-Shwartz and Ben-David's book <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/"><cite>Understanding Machine Learning: From Theory to Algorithms</cite></a></li>
            <li>(JWHT) refers to James, Witten, Hastie, and Tibshirani's book <a href="http://www-bcf.usc.edu/~gareth/ISL"><cite>An Introduction to Statistical Learning</cite></a></li>
        </ul>

        <section class="module" id="lecture-1-black-box-machine-learning">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-1-black-box-machine-learning">1. Black Box Machine Learning</a></h1>
                        <p>With the abundance of well-documented machine learning (ML) libraries, programmers can now "do" some ML, without any understanding of how things are working.  And we'll encourage such "black box" machine learning... just so long as you follow the procedures described in this lecture. To make proper use of ML libraries, you need to be conversant in the basic vocabulary, concepts, and workflows that underlie ML. We'll introduce the standard ML problem types (classification and regression) and discuss prediction functions, feature extraction, learning algorithms, performance evaluation, cross-validation, sample bias, nonstationarity, overfitting, and hyperparameter tuning.</p><p>If you're already familiar with standard machine learning practice, you can skip this lecture.</p>
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/MsD28INtSv8" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/01.black-box-ML.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                          (None)
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                          (None)
                    </td>
                    <td class="references">
                        <h1>References</h1>
                        <ul>
                                <li>Géron Ch 1,2</li>
                                <li><a href="http://www.data-science-for-biz.com/DSB/Home.html">Provost and Fawcett book</a></li>
                        </ul>
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-2-case-study-churn-prediction">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-2-case-study-churn-prediction">2. Case Study: Churn Prediction</a></h1>
                        <p>We have an interactive discussion about how to reformulate a real and subtly complicated business problem as a formal machine learning problem.  The real goal isn't so much to solve the problem, as to convey the point that properly mapping your business problem to a machine learning problem is both extremely important and often quite challenging.  This course doesn't dwell on how to do this mapping, though see Provost and Fawcett's book in the references.</p>
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/kE_t3Mm8Z50" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/churn.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                          (None)
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                          (None)
                    </td>
                    <td class="references">
                        <h1>References</h1>
                        <ul>
                                <li><a href="http://www.kdd.org/kdd-cup/view/kdd-cup-2009">KDD Cup 2009: Customer relationship prediction</a></li>
                                <li><a href="http://www.data-science-for-biz.com/DSB/Home.html">Provost and Fawcett book</a></li>
                        </ul>
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-3-introduction-to-statistical-learning-theory">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-3-introduction-to-statistical-learning-theory">3. Introduction to Statistical Learning Theory</a></h1>
                        This is where our "deep study" of machine learning begins. We introduce some of the core building blocks and concepts that we will use throughout the remainder of this course: input space, action space, outcome space, prediction functions, loss functions, and hypothesis spaces.  We present our first machine learning method: empirical risk minimization.  We also highlight the issue of overfitting, which may occur when we find the empirical risk minimizer over too large a hypothesis space.
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/rqJ8SrnmWu0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/02a.intro-stat-learning-theory.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                        <ul>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/Notes/conditional-expectations.pdf">Conditional Expectations</a></li>
                        </ul>
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                        <ul>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/1-Lec-Check.pdf">SLT and SGD Concept Check Questions</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/1-Lec-Check_sol.pdf">SLT and SGD Concept Check Solutions</a></li>
                                <li><a href="#assignment-homework-1">Homework 1: §3</a></li>
                        </ul>
                    </td>
                    <td class="references">
                        <h1>References</h1>
                          (None)
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-4-stochastic-gradient-descent">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-4-stochastic-gradient-descent">4. Stochastic Gradient Descent</a></h1>
                        A recurring theme in machine learning is that we formulate learning problems as optimization problems.  Empirical risk minimization was our first example of this.  To do learning, we need to do optimization. In this lecture we cover stochastic gradient descent, which is today's standard optimization method for large-scale machine learning problems.
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/5TZww5bTROE" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/02b.SGD.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                        <ul>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/Notes/directional-derivative.pdf">Directional Derivatives and Approximation (Short)</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/Labs/1-gradients-Notes_sol.pdf">Gradients and Directional Derivatives</a></li>
                        </ul>
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                        <ul>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/1-Lec-Check.pdf">SLT and SGD Concept Check Questions</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/1-Lec-Check_sol.pdf">SLT and SGD Concept Check Solutions</a></li>
                                <li><a href="#assignment-homework-1">Homework 1: §2</a></li>
                                <li><a href="#assignment-homework-2">Homework 2: §1,2</a></li>
                        </ul>
                    </td>
                    <td class="references">
                        <h1>References</h1>
                        <ul>
                                <li><a href="http://leon.bottou.org/papers/bottou-tricks-2012">Bottou's SGD Tricks</a></li>
                                <li><a href="http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf">Barnes "Matrix Differentiation" notes</a></li>
                        </ul>
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-5-excess-risk-decomposition">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-5-excess-risk-decomposition">5. Excess Risk Decomposition</a></h1>
                        We introduce the notions of approximation error, estimation error, and optimization error. While these concepts usually show up in more advanced courses, they will help us frame our understanding of the tradeoffs between hypothesis space choice, data set size, and optimization run times. In particular, these concepts will help us understand why "better" optimization methods (such as quasi-Newton methods) may not find prediction functions that generalize better, despite finding better optima.
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/YA_CE9jat4I" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/02c.excess-risk-decomposition.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                          (None)
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                        <ul>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/2-Lec-Check.pdf">Excess Risk and L1/L2 Questions</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/2-Lec-Check_sol.pdf">Excess Risk and L1/L2 Solutions</a></li>
                        </ul>
                    </td>
                    <td class="references">
                        <h1>References</h1>
                          (None)
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-6-l1-and-l2-regularization">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-6-l1-and-l2-regularization">6. L1 and L2 regularization</a></h1>
                        <p>We introduce "regularization", our main defense against overfitting. We discuss the equivalence of the penalization and constraint forms of regularization (see <a href="#assignment-homework-4">Hwk 4 Problem 8</a>), and we introduce L1 and L2 regularization, the two most important forms of regularization for linear models.  When L1 and L2 regularization are applied to linear least squares, we get "lasso" and "ridge" regression, respectively. We compare the "regularization paths" for lasso and ridge regression, and give a geometric argument for why lasso often gives "sparse" solutions. Finally, we present "coordinate descent", our second major approach to optimization.  When applied to the lasso objective function, coordinate descent takes a particularly clean form and is known as the "shooting algorithm".</p>
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/d6XDOS4btck" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/03a.L1L2-regularization.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                        <ul>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/Notes/completing-the-square.pdf">Completing the Square</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/2-PreLec-Check.pdf">Lasso Lecture Prep Questions</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/2-PreLec-Check_sol.pdf">Lasso Lecture Prep Solutions</a></li>
                        </ul>
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                        <ul>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/2-Lec-Check.pdf">Excess Risk and L1/L2 Questions</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/2-Lec-Check_sol.pdf">Excess Risk and L1/L2 Solutions</a></li>
                                <li><a href="#assignment-homework-2">Homework 2</a></li>
                        </ul>
                    </td>
                    <td class="references">
                        <h1>References</h1>
                        <ul>
                                <li>HTF 3.4</li>
                        </ul>
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-7-lasso-ridge-and-elastic-net">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-7-lasso-ridge-and-elastic-net">7. Lasso, Ridge, and Elastic Net</a></h1>
                        <p>We continue our discussion of ridge and lasso regression by focusing on the case of correlated features, which is a common occurrence in machine learning practice. We will see that ridge solutions tend to spread weight equally among highly correlated features, while lasso solutions may be unstable in the case of highly correlated features. Finally, we introduce the "elastic net", a combination of L1 and L2 regularization, which ameliorates the instability of L1 while still allowing for sparsity in the solution. (Credit to Brett Bernstein for the excellent graphics.)
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/KIoz_aa1ed4" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/03b.elastic-net.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                        <ul>
                                <li><a href="https://github.com/davidrosenberg/mlcourse/blob/gh-pages/Notebooks/Lasso%20and%20Elastic%20Net/lasso_and_elastic_net.ipynb">Lasso and Elastic Net (ipynb)</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/Notes/elastic-net-theorem.pdf">Elastic Net correlation theorem</a></li>
                        </ul>
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                        <ul>
                                <li><a href="#assignment-homework-2">Homework 2: §4.2,5</a></li>
                        </ul>
                    </td>
                    <td class="references">
                        <h1>References</h1>
                        <ul>
                                <li><a href="https://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&%20Hastie.pdf">Zou and Hastie's Elastic Net Paper (2005)</a></li>
                                <li><a href="https://arxiv.org/pdf/1411.3230v2.pdf">Mairal, Bach, and Ponce on Sparse Modeling</a></li>
                        </ul>
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-8-loss-functions-for-regression-and-classification">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-8-loss-functions-for-regression-and-classification">8. Loss Functions for Regression and Classification</a></h1>
                        We start by discussing absolute loss and Huber loss.  We consider them as alternatives to the square loss that are more robust to outliers.  Next, we introduce our approach to the classification setting, introducing the notions of score, margin, and margin-based loss functions. We discuss basic properties of the hinge loss (i.e SVM loss), logistic loss, and the square loss, considered as margin-based losses.  The interplay between the loss function we use for training and the properties of the prediction function we end up with is a theme we will return to several times during the course.
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/1oi_Mwozj5w" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/04a.loss-functions.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                          (None)
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                        <ul>
                                <li><a href="#assignment-homework-3">Homework 3: §2,3</a></li>
                                <li><a href="#assignment-homework-5">Homework 5: §2</a></li>
                        </ul>
                    </td>
                    <td class="references">
                        <h1>References</h1>
                          (None)
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-9-lagrangian-duality-and-convex-optimization">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-9-lagrangian-duality-and-convex-optimization">9. Lagrangian Duality and Convex Optimization</a></h1>
                        <p>We introduce the basics of convex optimization and Lagrangian duality. We discuss weak and strong duality, Slater's constraint qualifications, and we derive the complementary slackness conditions. As far as this course is concerned, there are really only two reasons for discussing Lagrangian duality: 1) The complementary slackness conditions will imply that SVM solutions are "sparse in the data" (<a href="#lecture-10-support-vector-machines">next lecture</a>), which has important practical implications for the kernelized SVMs (see the <a href="#lecture-13-kernel-methods">kernel methods lecture</a>). 2) Strong duality is a sufficient condition for the equivalence between the penalty and constraint forms of regularization (see <a href="#assignment-homework-4">Hwk 4 Problem 8</a>).</p> <p>This mathematically intense lecture may be safely skipped.</p>
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/thuYiebq1cE" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/04b.convex-optimization.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                        <ul>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/Notes/svm-lecture-prep.pdf">Pre-lecture warmup for SVM and Lagrangians</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/Notes/convex-optimization.pdf">Extreme Abridgment of BV</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/Lectures/04d.lagrangian-duality-in-ten-minutes.pdf">Lagrangian Duality (10-minute summary))</a></li>
                        </ul>
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                        <ul>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/4-Lec-Check.pdf">Subgradients and Lagrangian Duality Questions</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/4-Lec-Check_sol.pdf">Subgradients and Lagrangian Duality Solutions</a></li>
                                <li><a href="#assignment-homework-4">Homework 4: §8</a></li>
                        </ul>
                    </td>
                    <td class="references">
                        <h1>References</h1>
                          (None)
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-10-support-vector-machines">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-10-support-vector-machines">10. Support Vector Machines</a></h1>
                        <p>We define the soft-margin support vector machine (SVM) directly in terms of its objective function (L2-regularized, hinge loss minimization over a linear hypothesis space).  Using our knowledge of Lagrangian duality, we find a dual form of the SVM problem, apply the complementary slackness conditions, and derive some interesting insights into the connection between "support vectors" and margin. Read the "SVM Insights from Duality" in the Notes below for a high-level view of this mathematically dense lecture.</p> <details> <p><summary>More...</summary>Notably absent from the lecture is the hard-margin SVM and its standard geometric derivation. Although the derivation is fun, since we start from the simple and visually appealing idea of maximizing the "geometric margin", the hard-margin SVM is rarely useful in practice, as it requires separable data, which precludes any datasets with repeated inputs and label noise. One fixes this by introducing "slack" variables, which leads to a formulation equivalent to the soft-margin SVM we present. Once we introduce slack variables, I've personally found the interpretation in terms of maximizing the margin to be much hazier, and I find understanding the SVM in terms of "just" a particular loss function and a particular regularization to be much more useful for understanding its properties. That said, Brett Bernstein gives a very nice development of the geometric approach to the SVM, which is linked in the References below. At the very least, it's a great exercise in basic linear algebra.</p></details>
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/9zi6-RjlYrU" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/04c.SVM.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                        <ul>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/Notes/svm-lecture-prep.pdf">Pre-lecture warmup for SVM and Lagrangians</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/Notes/svm-notes.pdf">Support Vector Machines</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/Notes/SVM-main-points.pdf">SVM Insights from Duality</a></li>
                        </ul>
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                        <ul>
                                <li><a href="#assignment-homework-3">Homework 3</a></li>
                                <li><a href="#assignment-homework-4">Homework 4: §4</a></li>
                        </ul>
                    </td>
                    <td class="references">
                        <h1>References</h1>
                        <ul>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/Labs/3-SVM-Notes_sol.pdf">Geometric Derivation of SVMs</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/Labs/UniquenessOfSVM.pdf">Note on the Uniqueness of SVMs</a></li>
                        </ul>
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-11-subgradient-descent">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-11-subgradient-descent">11. Subgradient Descent</a></h1>
                        <p>Neither the lasso nor the SVM objective function is differentiable, and we had to do some work for each to optimize with gradient-based methods. It turns out, however, that gradient descent will essentially work in these situations, so long as you're careful about handling the non-differentiable points. To this end, we introduce "subgradient descent", and we show the surprising result that, even though the objective value may not decrease with each step, every step brings us closer to the minimizer.</p>  <p>This mathematically intense lecture may be safely skipped.</p>
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/jYtCiV1aP44" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/05a.subgradient-descent.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                        <ul>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/Labs/4-Subgradients-Notes_sol.pdf">Subgradients</a></li>
                        </ul>
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                        <ul>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/4-Lec-Check.pdf">Subgradients and Lagrangian Duality Questions</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/4-Lec-Check_sol.pdf">Subgradients and Lagrangian Duality Solutions</a></li>
                                <li><a href="#assignment-homework-3">Homework 3</a></li>
                        </ul>
                    </td>
                    <td class="references">
                        <h1>References</h1>
                        <ul>
                                <li><a href="http://web.stanford.edu/class/ee364b/lectures.html">Boyd's subgradient notes</a></li>
                        </ul>
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-12-feature-extraction">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-12-feature-extraction">12. Feature Extraction</a></h1>
                        <p>When using linear hypothesis spaces, one needs to encode explicitly any nonlinear dependencies on the input as features. In this lecture we discuss various strategies for creating features. Much of this material is taken, with permission, from Percy Liang's CS221 course at Stanford.</p>
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/gmli6EyiNRw" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/05b.features.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                        <ul>
                                <li><a href="https://github.com/davidrosenberg/mlcourse/blob/gh-pages/Notebooks/Features/simple_feature_transformations.ipynb">Simplest Example</a></li>
                                <li><a href="https://github.com/davidrosenberg/mlcourse/blob/gh-pages/Notebooks/Features/test_BOW.ipynb">Ingesting text with BOW</a></li>
                                <li><a href="https://github.com/davidrosenberg/mlcourse/blob/gh-pages/Notebooks/Features/polynomial_feature_comparison.ipynb">Polynomial features</a></li>
                                <li><a href="https://github.com/davidrosenberg/mlcourse/blob/gh-pages/Notebooks/Features/vector_quantization.ipynb">Vector quantization with k-means</a></li>
                        </ul>
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                        <ul>
                                <li><a href="#assignment-homework-3">Homework 3: §6,7,8</a></li>
                        </ul>
                    </td>
                    <td class="references">
                        <h1>References</h1>
                        <ul>
                                <li><a href="http://shop.oreilly.com/product/0636920049081.do">Feature Engineering for Machine Learning by Casari and Zheng</a></li>
                        </ul>
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-13-kernel-methods">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-13-kernel-methods">13. Kernel Methods</a></h1>
                        <p>With linear methods, we may need a whole lot of features to get a hypothesis space that's expressive enough to fit our data -- there can be orders of magnitude more features than training examples. While regularization can control overfitting, having a huge number of features can make things computationally very difficult, if handled naively.  For objective functions of a particular general form, which includes ridge regression and SVMs but not lasso regression, we can "kernelize", which can allow significant speedups in certain situations.  In fact, with the "kernel trick", we can even use an infinite-dimensional feature space at a computational cost that depends primarily on the training set size.</p> <details><p><summary>More...</summary>In more detail, it turns out that even when the optimal parameter vector we're searching for lives in a very high-dimensional vector space (dimension being the number of features), a basic linear algebra argument shows that for certain objective functions, the optimal parameter vector lives in a subspace spanned by the training input vectors. Thus, when we have more features than training points, we may be better off restricting our search to the lower-dimensional subspace spanned by training inputs. We can do this by an easy reparameterization of the objective function. This result is referred to as the "representer theorem", and its proof can be given on one slide.</p> <p>After reparameterization, we'll find that the objective function depends on the data only through the Gram matrix, or "kernel matrix", which contains the dot products between all pairs of training feature vectors. This is where things get interesting a second time: Suppose f is our featurization function. Sometimes the dot product between two feature vectors f(x) and f(x') can be computed much more efficiently than multiplying together corresponding features and summing. In such a situation, we write the dot products in terms of the "kernel function": k(x,x')=〈f(x),f(x')〉, which we hope to compute much more quickly than O(d), where d is the dimension of the feature space. The essence of a "kernel method" is to use this "kernel trick" together with the reparameterization described above.  This allows one to use huge (even infinite-dimensional) feature spaces with a computational burden that depends primarily on the size of your training set. In practice, it's useful for small and medium-sized datasets for which computing the kernel matrix is tractable. Scaling kernel methods to large data sets is still an active area of research.<p></details>
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/m1otj-SdwYw" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/06a.kernel-methods.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                          (None)
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                        <ul>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/5-Lab-Check.pdf">Kernel Concept Check Questions</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/5-Lab-Check_sol.pdf">Kernel Concept Check Solutions</a></li>
                                <li><a href="#assignment-homework-4">Homework 4</a></li>
                        </ul>
                    </td>
                    <td class="references">
                        <h1>References</h1>
                        <ul>
                                <li>SSBD Chapter 16</li>
                                <li><a href="http://homepages.rpi.edu/~bennek/class/mmld/papers/p49-gartner.pdf">A Survey of Kernels for Structured Data</a></li>
                        </ul>
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-14-performance-evaluation">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-14-performance-evaluation">14. Performance Evaluation</a></h1>
                        <p>This is our second "black-box" machine learning lecture. We start by discussing various models that you should almost ways build for your data, to use as baselines and performance sanity checks. From there we focus primarily on evaluating classifier performance.  We define a whole slew of performance statistics used in practice (precision, recall, F1, etc.). We also discuss the fact that most classifiers provide a numeric score, and if you need to make a hard classification, you should tune your threshold to optimize the performance metric of importance to you, rather than just using the default (typically 0 or 0.5). We also discuss the various performance curves you'll see in practice: precision/recall, ROC, and (my personal favorite) lift curves.</p>
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/xMyAL0C6cPY" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/06b.classifier-performance.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                          (None)
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                          (None)
                    </td>
                    <td class="references">
                        <h1>References</h1>
                        <ul>
                                <li>Provost and Fawcett book</li>
                        </ul>
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-15-citysense-probabilistic-modeling-for-unusual-behavior-detection">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-15-citysense-probabilistic-modeling-for-unusual-behavior-detection">15. &quot;CitySense&quot;: Probabilistic Modeling for Unusual Behavior Detection</a></h1>
                        <p>So far we have studied the regression setting, for which our predictions (i.e. "actions") are real-valued, as well as the classification setting, for which our score functions also produce real values. With this lecture, we begin our consideration of "conditional probability models", in which the predictions are probability distributions over possible outcomes. We motivate these models by discussion of the "CitySense" problem, in which we want to predict the probability distribution for the number of taxicab dropoffs at each street corner, at different times of the week. Given this model, we can then determine, in real-time, how "unusual" the amount of behavior is at various parts of the city, and thereby help you find the secret parties, which is of course the ultimate goal of machine learning.</p>
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/6nolrvzXiE4" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/08a.citysense.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                          (None)
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                          (None)
                    </td>
                    <td class="references">
                        <h1>References</h1>
                        <ul>
                                <li><a href="http://www1.cs.columbia.edu/~jebara/papers/CitySense.JSM2009.pdf">CitySense: multiscale space time clustering of GPS points and trajectories</a></li>
                        </ul>
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-16-maximum-likelihood-estimation">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-16-maximum-likelihood-estimation">16. Maximum Likelihood Estimation</a></h1>
                        In empirical risk minimization, we minimize the average loss on a training set. If our prediction functions are producing probability distributions, what loss functions will give reasonable performance measures? In this lecture, we discuss "likelihood", one of the most popular performance measures for distributions. We temporarily leave aside the conditional probability modeling problem, and focus on the simpler problem of fitting an unconditional probability model to data. We can use "maximum likelihood" to fit both parametric and nonparametric models. Once we have developed a collection of candidate probability distributions on training data, we select the best one by choosing the model that has highest "hold-out likelihood", i.e. likelihood on validation data.
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/ec_5vvxW7fE" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/08b.MLE.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                          (None)
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                        <ul>
                                <li><a href="#assignment-homework-5">Homework 5: §7,8</a></li>
                        </ul>
                    </td>
                    <td class="references">
                        <h1>References</h1>
                          (None)
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-17-conditional-probability-models">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-17-conditional-probability-models">17. Conditional Probability Models</a></h1>
                        In this lecture we consider prediction functions that produce distributions from a parametric family of distributions. We restrict to the case of linear models, though later in the course we will show how to make nonlinear versions using gradient boosting and neural networks. We develop the technique through four examples: Bernoulli regression (logistic regression being a special case), Poisson regression, Gaussian regression, and multinomial logistic regression (our first multiclass method). We conclude by connecting this maximum likelihood framework back to our empirical risk minimization framework.
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/JrFj0xpGd2Q" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/08c.conditional-probability-models.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                        <ul>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/Notes/conditional-exponential-distributions.pdf">Exponential Distribution Gradient Boosting (First part)</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/Notes/poisson-gradient-boosting.pdf">Poisson Gradient Boosting (First part)</a></li>
                        </ul>
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                        <ul>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/10-Lab-Check.pdf">Conditional Model Questions</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/10-Lab-Check_sol.pdf">Conditional Model Solutions</a></li>
                                <li><a href="#assignment-homework-5">Homework 5: §3</a></li>
                        </ul>
                    </td>
                    <td class="references">
                        <h1>References</h1>
                          (None)
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-18-bayesian-methods">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-18-bayesian-methods">18. Bayesian Methods</a></h1>
                        <p>We review some basics of classical and Bayesian statistics. For classical "frequentist" statistics, we define statistics and point estimators, and discuss various desirable properties of point estimators. For Bayesian statistics, we introduce the "prior distribution", which is a distribution on the parameter space that you declare before seeing any data. We compare the two approaches for the simple problem of learning about a coin's probability of heads. Along the way, we discuss conjugate priors, posterior distributions, and credible sets. Finally, we give the basic setup for Bayesian decision theory, which is how a Bayesian would go from a posterior distribution to choosing an action.</p>
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/VCfrGjDPC6k" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/09a.bayesian-methods.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                        <ul>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Notes/proportionality.pdf">Proportionality Review</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/in-prep/thompson-sampling-bernoulli.pdf">Thompson Sampling for Bernoulli Bandits [Optional]</a></li>
                        </ul>
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                        <ul>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/11-Lec-Check.pdf">Bayesian Methods and Regression Questions</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/11-Lec-Check_sol.pdf">Bayesian Methods and Regression Solutions</a></li>
                                <li><a href="#assignment-homework-5">Homework 5: §7,8</a></li>
                        </ul>
                    </td>
                    <td class="references">
                        <h1>References</h1>
                          (None)
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-19-bayesian-conditional-probability-models">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-19-bayesian-conditional-probability-models">19. Bayesian Conditional Probability Models</a></h1>
                        <p>In our earlier discussion of conditional probability modeling, we started with a hypothesis space of conditional probability models, and we selected a single conditional probability model using maximum likelihood or regularized maximum likelihood. In the Bayesian approach, we start with a prior distribution on this hypothesis space, and after observing some training data, we end up with a posterior distribution on the hypothesis space. For making conditional probability predictions, we can derive a predictive distribution from the posterior distribution. We explore these concepts by working through the case of Bayesian Gaussian linear regression. We also make a precise connection between MAP estimation in this model and ridge regression.</p>
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/Mo4p2B37LwY" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/09b.bayesian-regression.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                        <ul>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Notes/proportionality.pdf">Proportionality Review</a></li>
                        </ul>
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                        <ul>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/11-Lec-Check.pdf">Bayesian Methods and Regression Questions</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/11-Lec-Check_sol.pdf">Bayesian Methods and Regression Solutions</a></li>
                        </ul>
                    </td>
                    <td class="references">
                        <h1>References</h1>
                        <ul>
                                <li>Barber 9.1, 18.1</li>
                                <li>Bishop 3.3</li>
                        </ul>
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-20-classification-and-regression-trees">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-20-classification-and-regression-trees">20. Classification and Regression Trees</a></h1>
                        We begin our discussion of nonlinear models with tree models. We first describe the hypothesis space of decision trees, and we discuss some complexity measures we can use for regularization, including tree depth and the number of leaf nodes.  The challenge starts when we try to find the regularized empirical risk minimizer (ERM) over this space for some loss function. It turns out finding this ERM is computationally intractable. We discuss a standard greedy approach to tree building, both for classification and regression, in the case that features take values in any ordered set. We also describe an approach for handling categorical variables (in the binary classification case) and missing values.
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/GZuweldJWrM" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/10a.trees.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                        <ul>
                                <li><a href="https://github.com/davidrosenberg/mlcourse/blob/gh-pages/Contrib/trees/categorical_variables.ipynb">Categorical variables with trees (ipynb)</a></li>
                                <li><a href="https://github.com/davidrosenberg/mlcourse/blob/gh-pages/Contrib/trees/surrogate_split.ipynb">Missing data and surrogate splits (ipynb)</a></li>
                        </ul>
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                        <ul>
                                <li><a href="#assignment-homework-6">Homework 6: §6</a></li>
                        </ul>
                    </td>
                    <td class="references">
                        <h1>References</h1>
                        <ul>
                                <li>JWHT 8.1</li>
                                <li>HTF 9.2</li>
                                <li><a href="http://a.co/bNi2hH5">CART book by Breiman et al.</a></li>
                        </ul>
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-21-basic-statistics-and-a-bit-of-bootstrap">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-21-basic-statistics-and-a-bit-of-bootstrap">21. Basic Statistics and a Bit of Bootstrap</a></h1>
                        <p>In this lecture, we define bootstrap sampling and show how it is typically applied in statistics to do things such as estimating variances of statistics and making confidence intervals.  It can be used in a machine learning context for assessing model performance.</p>
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/lr5WH-JVT5I" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/10b.bootstrap.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                          (None)
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                          (None)
                    </td>
                    <td class="references">
                        <h1>References</h1>
                        <ul>
                                <li>JWHT 5.2 (Bootstrap)</li>
                                <li>HTF 7.11 (Bootstrap)</li>
                        </ul>
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-22-bagging-and-random-forests">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-22-bagging-and-random-forests">22. Bagging and Random Forests</a></h1>
                        <p>We motivate bagging as follows: Consider the regression case, and suppose we could create a bunch (say B) prediction functions based on independent training samples of size n. If we average together these prediction functions, the expected value of the average is the same as any one of the functions, but the variance would have decreased by a factor of 1/B -- a clear win! Of course, this would require an overall sample of size nB. The idea of bagging is to replace independent samples with bootstrap samples from a single data set of size n. Of course, the bootstrap samples are not independent, so much of our discussion is about when bagging does and does not lead to improved performance.  Random forests were invented as a way to create conditions in which bagging works better.</p> <details> <p><summary>More...</summary>Although it's hard to find crisp theoretical results describing when bagging helps, conventional wisdom says that it helps most for models that are "high variance", which in this context means the prediction function may change a lot when you train with a new random sample from the same distribution, and "low bias", which basically means fitting the training data well. Large decision trees have these characteristics and are usually the model of choice for bagging. Random forests are just bagged trees with one additional twist: only a random subset of features are considered when splitting a node of a tree. The hope, very roughly speaking, is that by injecting this randomness, the resulting prediction functions are less dependent, and thus we'll get a larger reduction in variance. In practice, random forests are one of the most effective machine learning models in many domains.</p></details>
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/f2S4hVs-ESw" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/10c.bagging-random-forests.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                        <ul>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/9-Lec-Check.pdf">Trees, Bootstrap, Bagging, and RF Questions</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/9-Lec-Check_sol.pdf">Trees, Bootstrap, Bagging, and RF Solutions</a></li>
                        </ul>
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                          (None)
                    </td>
                    <td class="references">
                        <h1>References</h1>
                        <ul>
                                <li>JWHT 8.2</li>
                                <li>HTF 8.7, 15, 10</li>
                                <li><a href="https://arxiv.org/pdf/1507.08502.pdf">A Conversation with Jerry Friedman</a></li>
                        </ul>
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-23-gradient-boosting">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-23-gradient-boosting">23. Gradient Boosting</a></h1>
                        <p>Gradient boosting is an approach to "adaptive basis function modeling", in which we learn a linear combination of M basis functions, which are themselves learned from a base hypothesis space H.  Gradient boosting may be used with any subdifferentiable loss function and over any base hypothesis space on which we can do regression. Regression trees are the most commonly used base hypothesis space. It is important to note that the "regression" in "gradient boosted regression trees" (GBRTs) refers to how we fit the basis functions, not the overall loss function.  GBRTs are routinely used for classification and conditional probability modeling. They are among the most dominant methods in competitive machine learning (e.g. Kaggle competitions).</p> <details> <p><summary>More...</summary>If the base hypothesis space H has a nice parameterization (say differentiable, in a certain sense), then we may be able to use standard gradient-based optimization methods directly. In fact, neural networks may be considered in this category. However, if the base hypothesis space H consists of trees, then no such parameterization exists.  This is where gradient boosting is really needed.</p> <p>For practical applications, it would be worth checking out the GBRT implementations in <a href="https://xgboost.ai/">XGBoost</a> and <a href="https://github.com/Microsoft/lightGBM">LightGBM</a>.</p> <p>See the Notes below for fully worked examples of doing gradient boosting for classification, using the hinge loss, and for conditional probability modeling using both exponential and Poisson distributions.  The code gbm.py illustrates L2-boosting and L1-boosting with decision stumps, for a one-dimensional regression dataset.</p> </details>
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/fz1H03ZKvLM" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/11a.gradient-boosting.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                        <ul>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/boosting-Lec-Check.pdf">Gradient Boosting Questions</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/boosting-Lec-Check_sol.pdf">Gradient Boosting Solutions</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/Notes/conditional-exponential-distributions.pdf">Exponential Distribution Gradient Boosting</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/Notes/poisson-gradient-boosting.pdf">Poisson Gradient Boosting</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/Archive/2017/Labs/gbm.py">gbm.py</a></li>
                        </ul>
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                        <ul>
                                <li><a href="#assignment-homework-6">Homework 6: §7,8</a></li>
                        </ul>
                    </td>
                    <td class="references">
                        <h1>References</h1>
                        <ul>
                                <li><a href="http://statweb.stanford.edu/~jhf/ftp/trebst.pdf">Friedman's GBM Paper</a></li>
                                <li><a href="http://www.saedsayad.com/docs/gbm2.pdf">Ridgeway's GBM Guide</a></li>
                                <li><a href="http://arxiv.org/abs/1603.02754">XGBoost Paper</a></li>
                        </ul>
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-24-multiclass-and-introduction-to-structured-prediction">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-24-multiclass-and-introduction-to-structured-prediction">24. Multiclass and Introduction to Structured Prediction</a></h1>
                        Here we consider how to generalize the score-producing binary classification methods we've discussed (e.g. SVM and logistic regression) to multiclass settings. We start by discussing "One-vs-All", a simple reduction of multiclass to binary classification. This usually works just fine in practice, despite the interesting failure case we illustrate. However, One-vs-All doesn't scale to a very large number of classes, since we have to train a separate model for each class. This is the real motivation for presenting the "compatibility function" approach described in this lecture. The approach presented here extends to structured prediction problems, where the output space may be exponentially large. We didn't have time to define structured prediction in the lecture, but please see the slides and the SSBD book in the references.
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/WMQwtoMUjDA" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/11b.multiclass.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                          (None)
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                        <ul>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/7-Lec-Check.pdf">Multiclass Questions</a></li>
                                <li><a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/7-Lec-Check_sol.pdf">Multiclass Solutions</a></li>
                                <li><a href="#assignment-homework-6">Homework 6: §1-5</a></li>
                        </ul>
                    </td>
                    <td class="references">
                        <h1>References</h1>
                        <ul>
                                <li>SSBD 17.1-17.3</li>
                                <li><a href="http://www.jmlr.org/papers/v5/rifkin04a.html">In Defense of One-vs-All Classification</a></li>
                                <li><a href="http://www.jmlr.org/papers/volume1/allwein00a/allwein00a.pdf">Reducing Multiclass to Binary</a></li>
                        </ul>
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-25-k-means-clustering">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-25-k-means-clustering">25. k-Means Clustering</a></h1>
                        Here we start our short unit on unsupervised learning. k-means clustering is presented first as an algorithm and then as an approach to minimizing a particular objective function. One challenge with clustering algorithms is that it's not obvious how to measure success.  (See Section 22.5 of the SSBD book for a nice discussion.) When possible, I prefer to take a probabilistic modeling approach, as discussed in the next two lectures.
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/J0A_tkIgutw" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/13a.k-means.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                          (None)
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                          (None)
                    </td>
                    <td class="references">
                        <h1>References</h1>
                        <ul>
                                <li>HTF 13.2.1</li>
                                <li>SSBD 22.2</li>
                        </ul>
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-26-gaussian-mixture-models">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-26-gaussian-mixture-models">26. Gaussian Mixture Models</a></h1>
                        A Gaussian mixture model (GMM) is a family of multimodal probability distributions, which is a plausible generative model for clustered data. We can fit this model using maximum likelihood, and we can assess the quality of fit by evaluating the model likelihood on holdout data. While the "learning" phase of Gaussian mixture modeling is fitting the model to data, in the "inference" phase, we determine for any point drawn from the GMM the probability that it came from each of the k components. To use a GMM for clustering, we simply assign each point to the component that it is most likely to have come from. k-means clustering can be seen as a limiting case of a restricted form of Gaussian mixture modeling.
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/I9dfOMAhsug" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/13b.mixture-models.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                          (None)
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                          (None)
                    </td>
                    <td class="references">
                        <h1>References</h1>
                        <ul>
                                <li>Bishop 9.2,9.3</li>
                                <li><a href="https://arxiv.org/pdf/1706.03267.pdf">An Alternative to EM for GMM [Optional]</a></li>
                        </ul>
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-27-em-algorithm-for-latent-variable-models">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-27-em-algorithm-for-latent-variable-models">27. EM Algorithm for Latent Variable Models</a></h1>
                        It turns out, fitting a Gaussian mixture model by maximum likelihood is easier said than done: there is no closed from solution, and our usual gradient methods do not work well. The standard approach to maximum likelihood estimation in a Gaussian mixture model is the expectation maximization (EM) algorithm. In this lecture, we present the EM algorithm in the general setting of latent variable models, of which GMM is a special case. We present the EM algorithm as a very basic "variational method" and indicate a few generalizations.
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/lMShR1vjbUo" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/13c.EM-algorithm.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                          (None)
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                          (None)
                    </td>
                    <td class="references">
                        <h1>References</h1>
                        <ul>
                                <li>Bishop 9.4</li>
                                <li><a href="http://www3.stat.sinica.edu.tw/statistica/oldpdf/a15n316.pdf">Vaida's "Parameter Convergence for EM and MM Algorithms"</a></li>
                        </ul>
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-28-neural-networks">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-28-neural-networks">28. Neural Networks</a></h1>
                        In the context of this course, we view neural networks as "just" another nonlinear hypothesis space. On the practical side, unlike trees and tree-based ensembles (our other major nonlinear hypothesis spaces), neural networks can be fit using gradient-based optimization methods. On the theoretical side, a large enough neural network can approximate any continuous function. We discuss the specific case of the multilayer perceptron for multiclass classification, which we view as a generalization of multinomial logistic regression from linear to nonlinear score functions.
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/Wr11D5sObzc" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/14a.neural-networks.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                          (None)
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                          (None)
                    </td>
                    <td class="references">
                        <h1>References</h1>
                        <ul>
                                <li><a href="http://neuralnetworksanddeeplearning.com/chap4.html">Michael Nielsen's chapter on universality of neural networks</a></li>
                        </ul>
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-29-backpropagation-and-the-chain-rule">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-29-backpropagation-and-the-chain-rule">29. Backpropagation and the Chain Rule</a></h1>
                        <p>Neural network optimization is amenable to gradient-based methods, but if the actual computation of the gradient is done naively, the computational cost can be prohibitive. Backpropagation is the standard algorithm for computing the gradient efficiently. We present the backpropagation algorithm for a general computation graph. The algorithm we present applies, without change, to models with "parameter tying", which include convolutional networks and recurrent neural networks (RNN's), the workhorses of modern computer vision and natural language processing. We illustrate backpropagation with one of the simplest models with parameter tying: regularized linear regression. Backpropagation for the multilayer perceptron, the standard introductory example, is presented in detail in <a href="#assignment-homework-7">Hwk 7 Problem 4</a>.</p>
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/XIpyEvLv93A" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/14b.backpropagation.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                          (None)
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                        <ul>
                                <li><a href="#assignment-homework-7">Homework 7</a></li>
                        </ul>
                    </td>
                    <td class="references">
                        <h1>References</h1>
                        <ul>
                                <li><a href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b">Yes you should understand backprop (Karpathy)</a></li>
                                <li><a href="https://youtu.be/gYpoJMlgyXA?t=13m44s">Challenges with backprop (Karpathy Lecture)</a></li>
                        </ul>
                    </td>
            </tr>
        </table>
        </section>
        <section class="module" id="lecture-30-next-steps">
            <table>
                <tr>
                    <td class="label" colspan="2">
                        <h1><a href="#lecture-30-next-steps">30. Next Steps</a></h1>
                        We point the direction to many other topics in machine learning that should be accessible to students of this course, but that we did not have time to cover.
                    </td>
                    <td colspan="2">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/RMmAVrhAfWs" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </td>
                </tr>
                <tr>
                    <td class="slides">
                        <h1>Slides</h1>
                        <a href="https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/14c.next-steps.pdf">Lecture Slides</a>
                    </td>
                    <td class="notes">
                        <h1>Notes</h1>
                          (None)
                    </td>
                    <td class="concept-checks">
                        <h1>Concept Checks</h1>
                          (None)
                    </td>
                    <td class="references">
                        <h1>References</h1>
                          (None)
                    </td>
            </tr>
        </table>
        </section>

    </section>
    <section id="assignments">
        <h1>Assignments</h1>

            <section class="homework" id="assignment-homework-1">
                <div class="module">
                    <div class="title">
                        <h1><a href="#assignment-homework-1">Homework 1</a></h1>
                        <p>GD, SGD, and Ridge Regression</p>
                    </div>
                    <div class="files">
                            <a href="https://davidrosenberg.github.io/mlcourse/Homework/hw1.pdf" class="pdf icon">hw1.pdf</a>
                            <a href="https://davidrosenberg.github.io/mlcourse/Homework/hw1.zip" class="zip icon">hw1.zip</a>
                    </div>
                </div>
            </section>
            <section class="homework" id="assignment-homework-2">
                <div class="module">
                    <div class="title">
                        <h1><a href="#assignment-homework-2">Homework 2</a></h1>
                        <p>Lasso Regression</p>
                    </div>
                    <div class="files">
                            <a href="https://davidrosenberg.github.io/mlcourse/Homework/hw2.pdf" class="pdf icon">hw2.pdf</a>
                            <a href="https://davidrosenberg.github.io/mlcourse/Homework/hw2.zip" class="zip icon">hw2.zip</a>
                    </div>
                </div>
            </section>
            <section class="homework" id="assignment-homework-3">
                <div class="module">
                    <div class="title">
                        <h1><a href="#assignment-homework-3">Homework 3</a></h1>
                        <p>SVM and Sentiment Analysis</p>
                    </div>
                    <div class="files">
                            <a href="https://davidrosenberg.github.io/mlcourse/Homework/hw3.pdf" class="pdf icon">hw3.pdf</a>
                            <a href="https://davidrosenberg.github.io/mlcourse/Homework/hw3.zip" class="zip icon">hw3.zip</a>
                    </div>
                </div>
            </section>
            <section class="homework" id="assignment-homework-4">
                <div class="module">
                    <div class="title">
                        <h1><a href="#assignment-homework-4">Homework 4</a></h1>
                        <p>Kernel Methods</p>
                    </div>
                    <div class="files">
                            <a href="https://davidrosenberg.github.io/mlcourse/Homework/hw4.pdf" class="pdf icon">hw4.pdf</a>
                            <a href="https://davidrosenberg.github.io/mlcourse/Homework/hw4.zip" class="zip icon">hw4.zip</a>
                    </div>
                </div>
            </section>
            <section class="homework" id="assignment-homework-5">
                <div class="module">
                    <div class="title">
                        <h1><a href="#assignment-homework-5">Homework 5</a></h1>
                        <p>Probabilistic Modeling</p>
                    </div>
                    <div class="files">
                            <a href="https://davidrosenberg.github.io/mlcourse/Homework/hw5.pdf" class="pdf icon">hw5.pdf</a>
                            <a href="https://davidrosenberg.github.io/mlcourse/Homework/hw5.zip" class="zip icon">hw5.zip</a>
                    </div>
                </div>
            </section>
            <section class="homework" id="assignment-homework-6">
                <div class="module">
                    <div class="title">
                        <h1><a href="#assignment-homework-6">Homework 6</a></h1>
                        <p>Multiclass, Trees, and Gradient Boosting</p>
                    </div>
                    <div class="files">
                            <a href="https://davidrosenberg.github.io/mlcourse/Homework/hw6.pdf" class="pdf icon">hw6.pdf</a>
                            <a href="https://davidrosenberg.github.io/mlcourse/Homework/hw6.zip" class="zip icon">hw6.zip</a>
                    </div>
                </div>
            </section>
            <section class="homework" id="assignment-homework-7">
                <div class="module">
                    <div class="title">
                        <h1><a href="#assignment-homework-7">Homework 7</a></h1>
                        <p>Computation Graphs, Backpropagation, and Neural Networks</p>
                    </div>
                    <div class="files">
                            <a href="https://davidrosenberg.github.io/mlcourse/Homework/hw7.pdf" class="pdf icon">hw7.pdf</a>
                            <a href="https://davidrosenberg.github.io/mlcourse/Homework/hw7.zip" class="zip icon">hw7.zip</a>
                    </div>
                </div>
            </section>
    </section>

    <section id="resources">
        <h1>Resources</h1>

        <section id="textbooks">
            <h1>Textbooks</h1>

            <a href="http://shop.oreilly.com/product/0636920052289.do"><img src="images/geron-original.jpg" alt="The cover of Hands-On Machine Learning with Scikit-Learn and TensorFlow"></a>

            <a href="https://web.stanford.edu/~hastie/ElemStatLearn/"><img src="images/hastie-1x.png" srcset="images/hastie-1x.png 1x, images/hastie-2x.jpg 2x, images/hastie-3x.jpg 3x" alt="The cover of Elements of Statistical Learning"></a>

            <a href="http://www-bcf.usc.edu/~gareth/ISL/"><img src="images/james-1x.jpg" srcset="images/james-1x.jpg 1x, images/james-2x.jpg 2x, images/james-3x.jpg 3x" alt="The cover of An Introduction to Statistical Learning"></a>

            <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/"><img src="images/shalev-shwartz-1x.jpg" srcset="images/shalev-shwartz-1x.jpg 1x, images/shalev-shwartz-2x.jpg 2x, images/shalev-shwartz-3x.jpg 3x" alt="The cover of Understanding Machine Learning: From Theory to Algorithms"></a>

            <a href="https://research.microsoft.com/en-us/um/people/cmbishop/PRML/"><img src="images/bishop-1x.jpg" srcset="images/bishop-1x.jpg 1x, images/bishop-2x.jpg 2x, images/bishop-3x.jpg 3x" alt="The cover of Pattern Recognition and Machine Learning"></a>

            <a href="http://www.data-science-for-biz.com/DSB/Home.html/"><img src="images/provost-fawcett-original.jpg" alt="The cover of Data Science for Business"></a>

            <dl>
                <dt><a href="http://shop.oreilly.com/product/0636920052289.do"><cite>Hands-On Machine Learning with Scikit-Learn and TensorFlow</cite> (Aurélien Géron)</a></dt>
                <dd>This is a practical guide to machine learning that corresponds fairly well with the content and level of our course.  While most of our homework is about coding ML from scratch with numpy, this book makes heavy use of scikit-learn and TensorFlow. We'll use the first two chapters of this book in the first two weeks of the course, when we cover "black-box machine learning."  It'll also be a handy reference for your projects and beyond this course, when you'll want to make use of existing ML packages, rather than rolling your own.</dd>

                <dt><a href="https://web.stanford.edu/~hastie/ElemStatLearn/"><cite>The Elements of Statistical Learning</cite> (Hastie, Friedman, and Tibshirani)</a></dt>
                <dd>This will be our main textbook for L1 and L2 regularization, trees, bagging, random forests, and boosting.  It's written by three statisticians who invented many of the techniques discussed. There's an easier version of this book that covers many of the same topics, described below. (Available for free as a PDF.)</dd>

                <dt><a href="http://www-bcf.usc.edu/~gareth/ISL/"><cite>An Introduction to Statistical Learning</cite> (James, Witten, Hastie, and Tibshirani)</a></dt>
                <dd>This book is written by two of the same authors as The Elements of Statistical Learning. It's much less intense mathematically, and it's good for a lighter introduction to the topics. Uses R as the language of instruction.  (Available for free as a PDF.)</dd>

                <dt><a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/"><cite>Understanding Machine Learning: From Theory to Algorithms</cite> (Shalev-Shwartz and Ben-David)</a></dt>
                <dd>This is our primary reference for kernel methods and multiclass classification, and possibly more towards the end of the course.  Covers a lot of theory that we don't go into, but it would be a good supplemental resource for a more theoretical course, such as Mohri's <a href="http://www.cs.nyu.edu/~mohri/ml16/">Foundations of Machine Learning</a> course. (Available for free as a PDF.)</dd>

                <dt><a href="https://research.microsoft.com/en-us/um/people/cmbishop/PRML/"><cite>Pattern Recognition and Machine Learning</cite> (Christopher Bishop)</a></dt>
                <dd>Our primary reference for probabilistic methods, including bayesian regression, latent variable models, and the EM algorithm.  It's highly recommended, but unfortunately not free online.</dd>

                <dt><a href="http://www.data-science-for-biz.com/DSB/Home.html"><cite>Data Science for Business</cite> (Provost and Fawcett)</a></dt>
                <dd>Ideally, this would be everybody's first book on machine learning.  The intended audience is both the ML practitioner and the ML product manager.  It's full of important core concepts and practical wisdom.  The math is so minimal that it's perfect for reading on your phone, and I encourage you to read it in parallel to doing this class.  Have your managers read it too.</dd>
            </dl>
        </section>

        <section id="references">
            <h1>Other tutorials and references</h1>

            <ul>
                <li><a href="http://www.cims.nyu.edu/~cfgranda/pages/DSGA1002_fall15/notes.html">Carlos Fernandez-Granda's lecture notes</a> provide a comprehensive review of the prerequisite material in linear algebra, probability, statistics, and optimization.</li>
                <li><a href="http://nbviewer.ipython.org/github/briandalessandro/DataScienceCourse/tree/master/ipython/">Brian Dalessandro's iPython notebooks</a> from <a href="https://github.com/briandalessandro/DataScienceCourse/blob/master/ipython/references/Syllabus_2017.pdf"><strong>DS-GA-1001: Intro to Data Science</strong></a></li>
                <li><a href="http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=3274">The Matrix Cookbook</a> has lots of facts and identities about matrices and certain probability distributions.</li>
                <li><a href="http://cs229.stanford.edu/section/cs229-prob.pdf">Stanford CS229: "Review of Probability Theory"</a></li>
                <li><a href="http://cs229.stanford.edu/section/cs229-linalg.pdf">Stanford CS229: "Linear Algebra Review and Reference"</a></li>
                <li><a href="http://www.umiacs.umd.edu/~hal/courses/2013S_ML/math4ml.pdf">Math for Machine Learning</a> by Hal Daumé III</li>
            </ul>
        </section>

    </section>



    <section id="people">
        <h1>People</h1>

        <section>
            <h1>Instructor</h1>

            <div class="person module instructor">
                <img src="images/people/david-1x.jpg" srcset="images/people/david-1x.jpg 1x, images/people/david-2x.jpg 2x, images/people/david-3x.jpg 3x" alt="A photo of David Rosenberg">
                <div class="info">
                    <p class="name"><a href="http://www.linkedin.com/pub/david-rosenberg/4/241/598">David S. Rosenberg</a></p>
                    <p class="email"><a href="mailto:drosenberg44@bloomberg.net">Email</a></p>
                    <p class="email"><a href="https://twitter.com/drosen">Twitter</a></p>
                    <p class="bio">David Rosenberg is a data scientist in the data science group in the Office of the CTO at <a href="https://www.techatbloomberg.com/post-topic/data-science/">Bloomberg</a>, and an adjunct associate professor at the Center for Data Science at New York University, where he has repeatedly received NYU's Center for Data Science "Professor of the Year" award. He received his Ph.D. in statistics from UC Berkeley, where he worked on statistical learning theory and natural language processing. David received a Master of Science in applied mathematics, with a focus on computer science, from Harvard University, and a Bachelor of Science in mathematics from Yale University.
                </div>
            </div>
        </section>

<!--
        <section class="multiple-people">
            <h1>Teaching Assistants</h1>

            <ul>

            </ul>
        </section>
-->
    </section>
    <script async defer src="scripts/navigation.js"></script>
</body>
<footer>
    <p>This website is developed <a href="https://github.com/bloomberg/foml/">on GitHub</a>.  Feel free to <a href="https://github.com/bloomberg/foml/issues">report issues or make suggestions</a>.</p>
</footer>
</html>
